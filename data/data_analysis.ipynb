{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from numpy import linalg as LA\n",
    "import csv\n",
    "sns.set_theme(style=\"darkgrid\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# current numpy and pandas versions have a FutureWarning for a mask operation we use;\n",
    "# we will ignore it\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "ROOT_DIRECTORY = Path(\"./\")\n",
    "DATA_DIRECTORY = ROOT_DIRECTORY\n",
    "\n",
    "DEFAULT_SUBMISSION_FORMAT = DATA_DIRECTORY / \"submission_format.csv\"\n",
    "DEFAULT_INPUT = DATA_DIRECTORY / \"ground_truth.csv\"\n",
    "DEFAULT_PARAMS = DATA_DIRECTORY / \"parameters.json\"\n",
    "DEFAULT_OUTPUT = ROOT_DIRECTORY / \"submission.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "submission_format = DEFAULT_SUBMISSION_FORMAT\n",
    "input_csv = DEFAULT_INPUT\n",
    "params_file = DEFAULT_PARAMS\n",
    "\n",
    "params = json.loads(params_file.read_text())\n",
    "ipums = pd.read_csv(input_csv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we want to understand how many users appear once, twice, ...\n",
    "appearances = ipums['sim_individual_id'].value_counts()\n",
    "\n",
    "sns.displot(appearances)\n",
    "\n",
    "appearances = appearances.values\n",
    "\n",
    "np.histogram(appearances, bins=range(1,21))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# bin the dataset\n",
    "BINS = {\n",
    "    \"AGE\": np.r_[-np.inf, np.arange(20, 105, 5), np.inf],\n",
    "    \"INCTOT\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"INCWAGE\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"INCWELFR\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"INCINVST\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"INCEARN\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"POVERTY\": np.r_[-np.inf, np.arange(0, 520, 20), np.inf],\n",
    "    \"HHWT\": np.r_[-np.inf, np.arange(0, 520, 20), np.inf],\n",
    "    \"PERWT\": np.r_[-np.inf, np.arange(0, 520, 20), np.inf],\n",
    "    \"DEPARTS\": np.r_[-np.inf, [h * 100 + m for h in range(24) for m in (0, 15, 30, 45)], np.inf],\n",
    "    \"ARRIVES\": np.r_[-np.inf, [h * 100 + m for h in range(24) for m in (0, 15, 30, 45)], np.inf],\n",
    "}\n",
    "\n",
    "bins_for_numeric_cols = BINS\n",
    "for col, bins in BINS.items():\n",
    "    ipums[col] = pd.cut(ipums[col], bins).cat.codes\n",
    "\n",
    "ipums.to_csv('binned_ground_truth.csv', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print domain size of each attribute\n",
    "with params_file.open(\"r\") as fp:\n",
    "    parameters = json.load(fp)\n",
    "\n",
    "attributes = columns = list(parameters[\"schema\"].keys())\n",
    "\n",
    "for att_i, att in enumerate(attributes):\n",
    "    num_bins = len(BINS[att]) if att in BINS else len(params['schema'][att]['values'])\n",
    "    print(att, num_bins)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# distribution of singletons\n",
    "single_attr_hist = {}\n",
    "for attr in attributes:\n",
    "    single_attr_hist[attr] = ipums[attr].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for attr in attributes:\n",
    "    single_attr_hist[attr].to_csv(f'single_{attr}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# distribution of pairs\n",
    "pair_attr_hist = {}\n",
    "for attr1, attr2 in itertools.combinations(attributes, 2):\n",
    "    # print(attr1, attr2)\n",
    "    groupby_df = ipums.groupby([attr1, attr2])\n",
    "    indices = pd.MultiIndex.from_product([ipums[col].unique() for col in [attr1, attr2]])\n",
    "    counts = groupby_df.size().to_frame('count').reindex(indices).fillna(0)\n",
    "\n",
    "    pair_attr_hist[(attr1, attr2)] = counts\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total = 1033968"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# L1 error of guessing\n",
    "# (by assuming independency of attributes and knowledge of singleton distrubtion)\n",
    "pair_attr_hist_ind = {}\n",
    "pair_attr_hist_l1 = np.zeros((len(attributes), len(attributes)))\n",
    "for attr1_i, attr1 in enumerate(attributes):\n",
    "    for attr2_i, attr2 in enumerate(attributes):\n",
    "        if attr1_i >= attr2_i:\n",
    "            continue\n",
    "\n",
    "        ground_truth = pair_attr_hist[(attr1, attr2)].sort_index()['count'].values\n",
    "\n",
    "        single1 = single_attr_hist[attr1].sort_index().values\n",
    "        single2 = single_attr_hist[attr2].sort_index().values\n",
    "\n",
    "        guess_ind = np.zeros_like(ground_truth)\n",
    "\n",
    "        for i, v in enumerate(single1):\n",
    "            guess_ind[i * len(single2) : (i + 1) * len(single2)] = v / total * single2\n",
    "\n",
    "        pair_attr_hist_ind[(attr1, attr2)] = guess_ind\n",
    "        pair_attr_hist_l1[attr1_i, attr2_i] = LA.norm(guess_ind - ground_truth, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.savetxt(\"pair_l1.csv\", pair_attr_hist_l1, delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the l1 errors in a 1d array\n",
    "pair_attr_hist_l1_1d = []\n",
    "for attr1_i, attr1 in enumerate(attributes):\n",
    "    for attr2_i, attr2 in enumerate(attributes):\n",
    "        if attr1_i >= attr2_i:\n",
    "            continue\n",
    "\n",
    "        ground_truth = pair_attr_hist[(attr1, attr2)]['count'].values\n",
    "        num_bins = 1\n",
    "        for att in [attr1, attr2]:\n",
    "            num_bins *= len(BINS[att]) if att in BINS else len(params['schema'][att]['values'])\n",
    "        row = [attr1, attr2, num_bins, np.count_nonzero(ground_truth), pair_attr_hist_l1[attr1_i, attr2_i]]\n",
    "        pair_attr_hist_l1_1d.append(row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"pair_l1_1d.csv\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     wr.writerows(pair_attr_hist_l1_1d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# understand l1 error of 3-way marginals fixing puma-year\n",
    "# similar to the previous one but treating puma and year as a single attribute\n",
    "puma_year_pair_attr_hist = {}\n",
    "for attr in attributes:\n",
    "    if attr in ['PUMA', 'YEAR']:\n",
    "        continue\n",
    "    # print(attr1, attr2)\n",
    "    groupby_df = ipums.groupby(['PUMA', 'YEAR', attr])\n",
    "    indices = pd.MultiIndex.from_product([ipums[col].unique() for col in ['PUMA', 'YEAR', attr]])\n",
    "    counts = groupby_df.size().to_frame('count').reindex(indices).fillna(0)\n",
    "\n",
    "    puma_year_pair_attr_hist[attr] = counts\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "puma_year_pair_attr_hist_ind = {}\n",
    "puma_year_pair_attr_hist_l1 = np.zeros((len(attributes), 3))\n",
    "single1 = pair_attr_hist[('PUMA', 'YEAR')].sort_index()['count'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for attr_i, attr in enumerate(attributes):\n",
    "\n",
    "    if attr in ['PUMA', 'YEAR']:\n",
    "        continue\n",
    "\n",
    "    ground_truth = puma_year_pair_attr_hist[attr].sort_index()['count'].values\n",
    "    num_bins = 181 * 7 * len(BINS[attr]) if attr in BINS else len(params['schema'][attr]['values'])\n",
    "\n",
    "    single2 = single_attr_hist[attr].sort_index().values\n",
    "\n",
    "    guess_ind = np.zeros_like(ground_truth)\n",
    "\n",
    "    for i, v in enumerate(single1):\n",
    "        guess_ind[i * len(single2) : (i + 1) * len(single2)] = v / total * single2\n",
    "\n",
    "    puma_year_pair_attr_hist_ind[attr] = guess_ind\n",
    "    puma_year_pair_attr_hist_l1[attr_i] = np.array([len(ground_truth), np.count_nonzero(ground_truth), LA.norm(guess_ind - ground_truth, 1)])\n",
    "\n",
    "np.savetxt(\"puma_year_pair_l1.csv\", puma_year_pair_attr_hist_l1, delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# L1 error of guessing\n",
    "# (by assuming independency of attributes and knowledge of singleton distribution)\n",
    "pair_attr_hist_ind = {}\n",
    "pair_attr_hist_l1 = np.zeros((len(attributes), len(attributes)))\n",
    "for attr1_i, attr1 in enumerate(attributes):\n",
    "    for attr2_i, attr2 in enumerate(attributes):\n",
    "        if attr1_i >= attr2_i:\n",
    "            continue\n",
    "\n",
    "        ground_truth = pair_attr_hist[(attr1, attr2)].sort_index()['count'].values\n",
    "\n",
    "        single1 = single_attr_hist[attr1].sort_index().values\n",
    "        single2 = single_attr_hist[attr2].sort_index().values\n",
    "\n",
    "        guess_ind = np.zeros_like(ground_truth)\n",
    "\n",
    "        for i, v in enumerate(single1):\n",
    "            guess_ind[i * len(single2) : (i + 1) * len(single2)] = v / total * single2\n",
    "\n",
    "        pair_attr_hist_ind[(attr1, attr2)] = guess_ind\n",
    "        pair_attr_hist_l1[attr1_i, attr2_i] = LA.norm(guess_ind - ground_truth, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.savetxt(\"pair_l1.csv\", pair_attr_hist_l1, delimiter=\",\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# understand the distance (l1 error of any pair) between the public data and any other state\n",
    "import os\n",
    "BINS = {\n",
    "    \"AGE\": np.r_[-np.inf, np.arange(20, 105, 5), np.inf],\n",
    "    \"INCTOT\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"INCWAGE\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"INCWELFR\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"INCINVST\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"INCEARN\": np.r_[-np.inf, np.arange(0, 105_000, 5_000), np.inf],\n",
    "    \"POVERTY\": np.r_[-np.inf, np.arange(0, 520, 20), np.inf],\n",
    "    \"HHWT\": np.r_[-np.inf, np.arange(0, 520, 20), np.inf],\n",
    "    \"PERWT\": np.r_[-np.inf, np.arange(0, 520, 20), np.inf],\n",
    "    \"DEPARTS\": np.r_[-np.inf, [h * 100 + m for h in range(24) for m in (0, 15, 30, 45)], np.inf],\n",
    "    \"ARRIVES\": np.r_[-np.inf, [h * 100 + m for h in range(24) for m in (0, 15, 30, 45)], np.inf],\n",
    "}\n",
    "\n",
    "def bin_attr(cur_df):\n",
    "    for col, bins in BINS.items():\n",
    "        cur_df[col] = cur_df.cut(ipums[col], bins)\n",
    "    return puma_df_i\n",
    "\n",
    "def obtain_two_way_marginal(cur_df):\n",
    "    cur_pair_attr_hist = {}\n",
    "    for attr1, attr2 in itertools.combinations(attributes, 2):\n",
    "        groupby_df = cur_df.groupby([attr1, attr2])\n",
    "        indices = pd.MultiIndex.from_product([cur_df[col].unique() for col in [attr1, attr2]])\n",
    "        counts = groupby_df.size().to_frame('count').reindex(indices).fillna(0)\n",
    "\n",
    "        cur_pair_attr_hist[(attr1, attr2)] = counts\n",
    "    return cur_pair_attr_hist\n",
    "\n",
    "pub_path = f'ground_truth.csv'\n",
    "pub_puma = pd.read_csv(pub_path)\n",
    "for col, bins in BINS.items():\n",
    "        pub_puma[col] = pd.cut(ipums[col], bins)\n",
    "\n",
    "state_puma = {}\n",
    "for i in range(55):\n",
    "    path_i = f'{i}-data.csv'\n",
    "    if os.path.isfile(path_i):\n",
    "        puma_df_i = pd.read_csv(path_i)\n",
    "        puma_df_i = bin_attr(puma_df_i)\n",
    "        state_puma[i] = puma_df_i\n",
    "\n",
    "pub_pair_attr_hist = obtain_two_way_marginal(pub_puma)\n",
    "state_pair_attr_hist = {}\n",
    "for state, state_df in state_puma.items():\n",
    "    state_pair_attr_hist[state] = obtain_two_way_marginal(state_df)\n",
    "\n",
    "attr_pairs = []\n",
    "for attr1_i, attr1 in enumerate(attributes):\n",
    "    if attr1 == 'PUMA':\n",
    "        continue\n",
    "    for attr2_i, attr2 in enumerate(attributes):\n",
    "        if attr2 == 'PUMA' or attr1_i >= attr2_i:\n",
    "            continue\n",
    "        attr_pairs.append(tuple([attr1, attr2]))\n",
    "\n",
    "output = np.array((len(state_puma), len(attr_pairs)))\n",
    "for state_i, state in enumerate(state_puma.keys()):\n",
    "    for attr_pair_i, attr_pair in enumerate(attr_pairs):\n",
    "        output[state_i][attr_pair_i] = LA.norm(pub_pair_attr_hist[attr_pair] - state_pair_attr_hist[state], 1)\n",
    "\n",
    "with open(\"states_pair_l1.csv\", 'w', newline='') as myfile:\n",
    "     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "     wr.writerows(pair_attr_hist_l1_1d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}